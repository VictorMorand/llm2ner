{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting NER Skills from LLMsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, sys, os, logging\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import circuitsvis as cv\n",
    "from importlib import reload\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# Import our own code\n",
    "import llm2ner\n",
    "import llm2ner.utils as utils\n",
    "import llm2ner.results as results\n",
    "import llm2ner.plotting as plotting\n",
    "from llm2ner.models import NERmodel\n",
    "from experimaestro import settings\n",
    "\n",
    "if (wspace := settings.get_workspace('LLMinterp')) is not None:\n",
    "    print(f\"Found experimaestro workspace: {wspace.id}\")\n",
    "    xp_path =  wspace.path\n",
    "    print(f\"xp_path: {xp_path}\")\n",
    "else:\n",
    "    xp_path = None\n",
    "    print(\"No experimaestro workspace found.\")\n",
    "\n",
    "USER = os.environ.get(\"USER\")\n",
    "print(f\"USER is {USER}\")\n",
    "if USER == \"morand\":\n",
    "    data_path = Path(\"/data/morand/NER\") \n",
    "elif USER == \"victor\": \n",
    "    data_path = Path(\"/Users/victor/code/data/NER\")\n",
    "else:\n",
    "    logging.error(f\"Unknown user {USER}, please set data_path manually.\")\n",
    "    data_path = None\n",
    "\n",
    "repo_path = Path(llm2ner.__file__).parent.parent.parent\n",
    "os.chdir(repo_path)\n",
    "\n",
    "print(f\"working dir: {os.getcwd()}\")\n",
    "print(f\"cuda available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use a saved model path directly:\n",
    "# path = \"/home/morand/experiments/llminterp/xp/llm2ner_TM/results/models/dataset_name=Pile-NER_dilate_entities=None_layer=5_llm_name=meta-llama-Llama-3.2-1B_method=cl_fn_minmaxpool_pos_weight=0_rank=64_teacher_thr_prob=0.9\"\n",
    "path = next(Path(\"./saved_models\").glob(\"ToMMeR*\"))\n",
    "\n",
    "print(f\"Loading learner from {path}\")\n",
    "\n",
    "tommer = NERmodel.from_pretrained(path)\n",
    "model_name = tommer.llm_name\n",
    "print(f\"Ner Model {tommer}\\n with {tommer.count_parameters()/1e3:.2f} K parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM\n",
    "Now we can load the backbone LLM that our `ToMMeR` was trained on.\n",
    "Note that while we only use hidden states from an early layer, we cut the LLM to this layer in order to save GMU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.load_llm(\n",
    "    model_name, to_hookedtransformer=tommer.need_hookedtransformer,\n",
    "    # dtype=torch.bfloat16,\n",
    "    cut_to_layer=tommer.layer,\n",
    ")\n",
    "device = next(model.parameters()).device\n",
    "tommer.to(device)\n",
    "c_length = utils.get_model_max_length(model_name)\n",
    "dim = utils.get_model_dim(model_name)\n",
    "print(f\"Model dimension is {dim}, context length is {c_length}\")\n",
    "#print current gpu mem\n",
    "print(f\"{model_name} loaded on {device} as {model.__class__.__name__} with {utils.count_parameters(model)/1e9:.3f} B parameters\")\n",
    "print(f\"GPU allocated memory : {torch.cuda.memory_allocated()/1024**3:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inference on any String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plotting)\n",
    "import llm2ner.plotting as plotting\n",
    "\n",
    "text = \"__ dans un rapport publié le 12 mars, le Health Data Hub fait le point sur sa procédure de migration de Microsoft Azure vers une offre \\\"souveraine\\\". En 2024, elle raconte avoir intensifié ses échanges avec les industriels et compte mettre en oeuvre une \\\"solution intercalaire\\\" en 2025.\"\n",
    "text = \"The Eiffel Tower is located near the Seine river, in Paris. It was built in 1889 by Gustave Eiffel.\"\n",
    "text = \"Selon les informations de la Cellule investigation de Radio France et du journal \\\"Le Monde\\\", la Ligue des droits de l'homme (LDH) vient de transmettre un signalement doublé d'une plainte au parquet de Paris visant l'assistant vocal de la marque à la pomme. Des accusations que la marque à la pomme a toujours réfutées mais contre lesquelles elle s’apprête tout de même à ouvrir un fonds d'indemnisation de 95 millions de dollars afin d’empêcher toute nouvelle procédure à son encontre aux Etats-Unis. S’il est validé par la justice californienne, l’accord amiable prévoit que les propriétaires américains d’iPhone, iPad, Apple Watch, MacBook, iMac, HomePod, iPod touch ou AppleTV pourront être indemnisés d’une somme de 20 dollars par appareil possédé.\"\n",
    "text = \"Large language models are awesome. While trained on language modeling, they exhibit emergent abilities that make them suitable for a wide range of tasks, including Named Entity Recognition (NER). \"\n",
    "text = \"Our work aims to bridge this gap, connecting known theoretical results in geometric algebra with modern advances in neural information retrieval. We draw upon research in communication complexity theory to provide a lower bound on the embedding dimension needed to represent a given combination of relevant documents and queries. Specifically, we show that for a given embedding dimension d there exists top-k combinations of documents that cannot be returned—no matter the query—highlighting a theoretical and fundamental limit to embedding models.\"\n",
    "\n",
    "outputs = plotting.demo_inference(text, tommer, model, \n",
    "                                #   decoding_strategy=\"greedy\", \n",
    "                                #   threshold=0.7, \n",
    "                                  return_logits=False,\n",
    "                                  show_attn=True,\n",
    "                                #   show_values=True,\n",
    "                                #   verbose=True,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_name = \"Ontonotes\"\n",
    "eval_data_name = \"CoNLL2003\"\n",
    "eval_data_name = \"WikiNeural\"\n",
    "eval_data_name = \"WikiANN en\"\n",
    "eval_data_name = \"CrossNER_literature\"\n",
    "eval_data_name = \"ncbi\"\n",
    "eval_data_name = \"GENIA_NER\"\n",
    "eval_data_name = \"CrossNER_AI\"\n",
    "eval_data_name = \"CrossNER_politics\"\n",
    "\n",
    "decoding_strategy = \"threshold\" # threshold greedy\n",
    "threshold = 0.5\n",
    "\n",
    "eval_dataset = llm2ner.data.load_all_splits(eval_data_name, data_folder=Path(\"/data/morand/NER\"), mode=\"last\", model=model)\n",
    "logging.info(f\"\\n{len(eval_dataset)} samples in {eval_data_name}\")\n",
    "logging.info(f\"Evaluating at layer {tommer.layer} of {tommer.llm_name}, Computing metrics on test set of {eval_data_name}...\")\n",
    "tommer = tommer.cuda()\n",
    "metrics = tommer.evaluate(eval_dataset.get_loader(batch_size=30), decoding_strategy=decoding_strategy, threshold=threshold, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plotting)\n",
    "from llm2ner import plotting\n",
    "\n",
    "data = eval_dataset\n",
    "id = torch.randint(len(data), (1,)).item()\n",
    "item = data[id]\n",
    "print(id)\n",
    "\n",
    "outputs = plotting.test_inference(\n",
    "    tommer,\n",
    "    model,\n",
    "    item,\n",
    "    # decoding_strategy=\"greedy\", \n",
    "    threshold=0.5,\n",
    "    show_attn=True,\n",
    "    # verbose=True,\n",
    "    return_logits=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
