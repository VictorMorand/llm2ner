id: NERfromLLM_CNN
title: "Training"
description: "Training a custom low rank self attention layer to extract NER skills from LLMs"
add_timestamp: true
file: xp_learnAttnCNN
pythonpath: 
    - .././

# General parameters
# model_name: "gpt2-medium" # 24 (0-23) Transformer layers 
# model_name: "meta-llama/Llama-3.2-1B"  # 16 layers 1.1B
# model_name: "mistral-7b"  # 7B
# model_name: "phi-2" # 32 (0-31) Transformer layers | ok 32G with batch 32
# model_name: "phi-1_5" # 24 (0-23) Transformer layers | 
model_name: "meta-llama/Llama-3.2-1B"  # 16 layers 1.1B
# model_name: "meta-llama/Llama-3.2-3B" # 28 layers 3.2 B | OK 32G - batch 8


launcher: "duration=5h & cuda(mem=30G)*1 & cpu(cores=8)"

#other model params
rank: 64
mask_bos: True
sliding_window: 30
kernel_padding: [0, 1, 0 , 0 ] # left, right, top, bottom

# Data
dataset_name: "Pile-NER" # need to define the data folder
# dataset_name: "CoNLL2003"
data_folder: "/lustre/fswork/projects/rech/cdt/uqo57sk/data/NER" # on JZ
# data_folder: "/data/morand/NER" # on hacienda
max_length: 2000
max_ent_length: 100

# Training
epochs: 2               # number of epochs
batch_size: 8
lr: 1e-2
lasso_reg: 1e-5           # lasso regularization
patience: 100             # patience for lr scheduler
n_val: 3000             # number of samples between validation and logging
accumulation_steps: 6   # 1 for no accumulation
grad_clip: 20.0          # 0 for no clipping
val_metric: "recall"        # metric to use for validation

# Grid search parameters
layers: 
    from: 5
    to: 15

methods:                # Aggregation methods
    - "inter"   
    # - "logits"
    # - "mean"

pos_weight:             # weight for positive class in BCEWithLogitsLoss
    - 30.0 
    - 40.0 
    # - 20.0

dilate_entities:         # train only on entities, 
    # - [0]              # None - False
    - [3,3]             # 5 - True
    # - [3]                 # 3 - True
    # - [1]                 # 3 - True
    - [2]                 # 3 - True

### Evaluation
eval_datasets:
    - "MultiNERD" 
    - "CoNLL 2003"
    - "CrossNER_politics"
    - "CrossNER_AI"
    - "CrossNER_literature"
    - "CrossNER_science"
    - "CrossNER_music"
    - "FabNER"
    - "mit-restaurant"
    - "ncbi"
    - "WikiNeural"