id: NERfromLLM_CLQK          #id of the experiment
title: "Training"
description: "Training a custom low rank self attention layer to extract NER skills from LLMs"
add_timestamp: true
file: xp_learnCLKQner
pythonpath: 
    - .././

# General parameters
# model_name: "gpt2-medium" # 24 (0-23) Transformer layers 
# model_name: "meta-llama/Llama-3.2-1B"  # 16 layers 1.1B
# model_name: "mistral-7b"  # 7B
# model_name: "phi-2" # 32 (0-31) Transformer layers | ok 32G with batch 32
# model_name: "phi-1_5" # 24 (0-23) Transformer layers | 
# model_name: "EleutherAI/pythia-410m" # 28 layers 3.2 B | OK 16Gb Pile-NER
model_name: "meta-llama/Llama-3.2-1B"  # 16 layers 1.1B | OK 32Gb Pile-NER (19Bg max batch size 8)
# model_name: "meta-llama/Llama-3.2-3B" # 28 layers 3.2 B


launcher: "duration=6h & cuda(mem=16G)*1 & cpu(cores=4)" # 4cores per 24Gb GPU on Hacienda..


###############  Dataset ###############

# dataset_name: "MultiNERD" # need to define the data folder
dataset_name: "Pile-NER" # need to define the data folder
# dataset_name: "CoNLL2003"
# data_folder: "/lustre/fswork/projects/rech/cdt/uqo57sk/data/NER" # on JZ
data_folder: "/data/morand/NER" # on hacienda

max_length: 1500    # max length of the input sequence
max_ent_length: 400  # max length of entity in characters

############### Training ###############

epochs: 2                   # number of epochs
batch_size: 8               # batch size
lr: 1e-2                    # learning rate 
patience: 50                # patience for lr scheduler
n_val: 5000                 # number of samples between validation and logging
accumulation_steps: 10      # 1 for no accumulation
grad_clip: 20.0             # 0 for no clipping
val_metric: "recall"        # metric to use for validation
pos_weight:                 # weight for positive class in BCEWithLogitsLoss
    - 0.0
#     - 30.0 
    # - 10.0 
    # - 40.0 

self_distillation_phases: 3
sparse_distill_loss: true
teacher_thr_prob: 0.99

############### Model ###############

rank: 64
sliding_window: 25
layers:                     # will extract Q and K from layer [range( l-n: l ) for l in range( from : to )]
    from: 5               #cannot do -1 for CLQK ! 
    to: 5
    n: 1

use_cosine: true
normalize_scores: ""    # atan , log_sigmoid

methods:                    # Aggregation methods, see models.METHODS
    # - "inter_first"   
    # - "inter_first_soft"   
    # - "inter_first_neg"   
    # - "cl_first_mean"   
    # - "cl_prev_first_mean"   
    # - "cl_first_next_mean"   
    - "cl_first_next_pool"   
    - "cl_first_next_minpool"   
    - "cl_fn_minmaxpool"   

# dilate_entities:         # train only on entities, 
#     # - [0]               # 0 - False
#     - [2]                 
#     - [3]                 
    # - [1]                 
    # - [3,3]                 
    # - [2,2]                 

############### Evaluation ###############

eval_datasets:
    - "MultiNERD" 
    - "CoNLL 2003"
    - "CrossNER_politics"
    - "CrossNER_AI"
    - "CrossNER_literature"
    - "CrossNER_science"
    - "CrossNER_music"
    - "FabNER"
    - "ncbi"
    - "WikiNeural"