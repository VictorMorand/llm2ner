id: NERfromLLM
title: "Training"
description: "Training a custom low rank self attention layer to extract NER skills from LLMs"
file: xp_learnAttn
pythonpath: 
    - .././

# General parameters
# model_name: "phi-1_5" # 24 (0-23) Transformer layers | 
# model_name: "phi-2" # 32 (0-31) Transformer layers | ok 32G with batch 32
# model_name: "meta-llama/Llama-3.2-3B" # 28 layers 3.2 B
# model_name: "gpt2-medium" # 24 (0-23) Transformer layers 
model_name: "meta-llama/Llama-3.2-1B"  # 16 layers 1.1B

launcher: "duration=2h & cuda(mem=20G)*1 & cpu(cores=8)"

rank: 100
layers: 
    from: 5
    to: 10

# Data
# dataset_name: "CoNLL2003"
dataset_name: "MultiNERD" # need to define the data folder
# data_folder: "/lustre/fswork/projects/rech/cdt/uqo57sk/data/NER" # on JZ
data_folder: "/data/morand/NER" # on hacienda


mode:  
    - "last"
    # - "block"        
    # - "full"
    # - "block_only"        

causal_mask:
    # - False
    - True

mask_bos: True

max_length: 200
max_ent_length: 20

# Training
epochs: 2               # number of epochs
batch_size: 32
lr: 1e-2
patience: 20             # patience for lr scheduler
n_val: 3000             # number of samples between validation and logging
val_metric: "recall"        # metric to use for validation
pos_weight:             # weight for positive class in BCEWithLogitsLoss
    - 10.0       
    - 30.0 
accumulation_steps: 5   # 1 for no accumulation
grad_clip: 10.0          # 0 for no clipping
dilate_entities:        # train only on entities, 
    # - [0]              # None - False
    # - [3,3]             # 5 - True
    - [3]                 # 3 - True